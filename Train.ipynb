{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e9aa3c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import Dataset4Pandas\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from model import Model\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590d6114",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e87debd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/home/koynov/sentimentanalysistask/data/training_with_cluster_lbls.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c91ce11",
   "metadata": {},
   "outputs": [],
   "source": [
    "unclustered = data[data[\"augmentation_cluster_lbl\"]==-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ef2cd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered = data[data[\"augmentation_cluster_lbl\"]!=-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bb80dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_clusters = clustered[\"company_cluster_lbl\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bdd0142",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clusters, test_clusters = train_test_split(unique_clusters, random_state = 42,test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f67cbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_unclustered, test_unclustered = train_test_split(unclustered, random_state=42, stratify= unclustered.label,test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45db9112",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clustered = clustered[clustered[\"company_cluster_lbl\"].isin(train_clusters)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3957f241",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_clustered = clustered[clustered[\"company_cluster_lbl\"].isin(test_clusters)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f0dd9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([train_clustered, train_unclustered])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4ce717a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.concat([test_clustered, test_unclustered])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd36984",
   "metadata": {},
   "source": [
    "## Create Dataset Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9dd0aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset4Pandas(train, \n",
    "                     text_column=\"text\", \n",
    "                     label_column=\"encoded_label\",\n",
    "                     company_column=\"company\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad094d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = Dataset4Pandas(test, \n",
    "                     text_column=\"text\", \n",
    "                     label_column=\"encoded_label\",\n",
    "                     company_column=\"company\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5de3c0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = DataLoader(train_ds, collate_fn=Dataset4Pandas.collate_fn, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6742a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_test = DataLoader(test_ds, collate_fn=Dataset4Pandas.collate_fn, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941115ee",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65a44626",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "688fd447",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = Model(companies_list=list(train.company.unique())).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c676d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr = 3.5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c9e7eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "99949bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training epoch: 1/3: 100%|██████████| 13/13 [00:44<00:00,  3.44s/it, acc=0.404, loss=1.34]\n",
      "testing epoch: 1/3: 100%|██████████| 13/13 [00:09<00:00,  1.41it/s, acc=0.337, loss=1.4] \n",
      "training epoch: 2/3: 100%|██████████| 13/13 [00:30<00:00,  2.37s/it, acc=0.404, loss=1.31]\n",
      "testing epoch: 2/3: 100%|██████████| 13/13 [00:08<00:00,  1.61it/s, acc=0.298, loss=1.47]\n",
      "training epoch: 3/3: 100%|██████████| 13/13 [00:28<00:00,  2.23s/it, acc=0.462, loss=1.19]\n",
      "testing epoch: 3/3: 100%|██████████| 13/13 [00:08<00:00,  1.59it/s, acc=0.269, loss=1.44]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs+1):\n",
    "    pbar_train = tqdm(enumerate(dataloader_train), total = len(dataloader_train))\n",
    "    pbar_train.set_description(f\"training epoch: {epoch}/{epochs}\")\n",
    "    \n",
    "    losses_in_epoch = []\n",
    "    acc_in_epoch = []\n",
    "    losses_in_epoch_test = []\n",
    "    acc_in_epoch_test = []\n",
    "    model.train()\n",
    "    for i, (texts, labels, company_names) in pbar_train:\n",
    "        labels = labels.to(device)\n",
    "        texts = texts.to(device)\n",
    "        \n",
    "        preds = model(texts, company_names)\n",
    "        loss_ = loss(preds, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss_.backward()\n",
    "        optimizer.step()\n",
    "        losses_in_epoch.append(loss_.item())\n",
    "        \n",
    "        acc = (preds.argmax(1)==labels).type(torch.float32).mean()\n",
    "        acc_in_epoch.append(acc.item())\n",
    "        pbar_train.set_postfix(loss = sum(losses_in_epoch)/(i+1), acc = sum(acc_in_epoch)/(i+1))\n",
    "    model.eval()\n",
    "    pbar_test = tqdm(enumerate(dataloader_test), total = len(dataloader_test), leave = True)\n",
    "    pbar_test.set_description(f\"testing epoch: {epoch}/{epochs}\")\n",
    "    for i, (texts, labels, company_names) in pbar_test:\n",
    "        labels = labels.to(device)\n",
    "        texts = texts.to(device)\n",
    "        \n",
    "        preds = model(texts, company_names)\n",
    "        loss_ = loss(preds, labels)\n",
    "  \n",
    "        losses_in_epoch_test.append(loss_.item())\n",
    "        \n",
    "        acc = (preds.argmax(1)==labels).type(torch.float32).mean()\n",
    "        acc_in_epoch_test.append(acc.item())\n",
    "        \n",
    "        pbar_test.set_postfix(loss = sum(losses_in_epoch_test)/(i+1), acc = sum(acc_in_epoch_test)/(i+1))\n",
    "    torch.save(model.state_dict(), f\"saved_checkpoint_{epoch}.ckpt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10351488",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a9a7a22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = pd.read_csv(\"./data/validation_with_encoded_lbl.csv\").dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "31446aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = Dataset4Pandas(val,\n",
    "                             text_column=\"text\", \n",
    "                             label_column=\"encoded_label\",\n",
    "                             company_column=\"company\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "504625de",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader =  DataLoader(val_dataset, collate_fn=Dataset4Pandas.collate_fn, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6d2f83dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "testing epoch: 3/3: 100%|██████████| 13/13 [00:12<00:00,  1.01it/s, acc=0.375, loss=1.37]\n"
     ]
    }
   ],
   "source": [
    "pbar_val = tqdm(enumerate(val_dataloader), total = len(val_dataloader), leave = True)\n",
    "pbar_val.set_description(f\"testing epoch: {epoch}/{epochs}\")\n",
    "predictions = []\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "model.eval()\n",
    "for i, (texts, labels, company_names) in pbar_val:\n",
    "    labels = labels.to(device)\n",
    "    texts = texts.to(device)\n",
    "\n",
    "    preds = model(texts, company_names)\n",
    "    loss_ = loss(preds, labels)\n",
    "\n",
    "    val_loss.append(loss_.item())\n",
    "    preds = preds.argmax(1).detach()#.cpu().numpy()\n",
    "    acc = (preds==labels).type(torch.float32).mean()\n",
    "    val_acc.append(acc.item())\n",
    "    predictions.append(preds.cpu().numpy())\n",
    "    pbar_val.set_postfix(loss = sum(val_loss)/(i+1), acc = sum(val_acc)/(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c54c147e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Negative</th>\n",
       "      <th>Neutral</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Irrelevant</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>macro avg</th>\n",
       "      <th>weighted avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.173077</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.348464</td>\n",
       "      <td>0.352687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.296296</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.285143</td>\n",
       "      <td>0.280000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1-score</th>\n",
       "      <td>0.243243</td>\n",
       "      <td>0.243902</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.326531</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.286752</td>\n",
       "      <td>0.287530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>support</th>\n",
       "      <td>22.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0.28</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Negative    Neutral   Positive  Irrelevant  accuracy   macro avg  \\\n",
       "precision   0.173077   0.357143   0.500000    0.363636      0.28    0.348464   \n",
       "recall      0.409091   0.185185   0.250000    0.296296      0.28    0.285143   \n",
       "f1-score    0.243243   0.243902   0.333333    0.326531      0.28    0.286752   \n",
       "support    22.000000  27.000000  24.000000   27.000000      0.28  100.000000   \n",
       "\n",
       "           weighted avg  \n",
       "precision      0.352687  \n",
       "recall         0.280000  \n",
       "f1-score       0.287530  \n",
       "support      100.000000  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(classification_report(val_dataset.dataframe.encoded_label.values, np.hstack(predictions), output_dict=True))\\\n",
    "                    .rename(columns = {\"0\": 'Negative', \"1\": 'Neutral', \"2\": 'Positive', \"3\": 'Irrelevant'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32328954",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220d455d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
